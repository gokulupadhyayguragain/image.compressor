name: Build, push Docker images to ECR and deploy to EKS

on:
  push:
    branches: [ main ]

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Ensure ECR repositories exist
        run: |
          set -e
          # Create backend repo if missing
          aws ecr describe-repositories --repository-names "${{ secrets.ECR_REPOSITORY_BACKEND }}" >/dev/null 2>&1 || aws ecr create-repository --repository-name "${{ secrets.ECR_REPOSITORY_BACKEND }}"
          # Create frontend repo if missing
          aws ecr describe-repositories --repository-names "${{ secrets.ECR_REPOSITORY_FRONTEND }}" >/dev/null 2>&1 || aws ecr create-repository --repository-name "${{ secrets.ECR_REPOSITORY_FRONTEND }}"

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build and push backend image
        uses: docker/build-push-action@v4
        with:
          # Use repository root as context because the Dockerfile expects files under ./backend/
          context: .
          file: ./docker/backend.dockerfile
          push: true
          tags: ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_BACKEND }}:latest

      - name: Build and push frontend image
        uses: docker/build-push-action@v4
        with:
          context: .
          file: ./docker/frontend.dockerfile
          push: true
          tags: ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_FRONTEND }}:latest

  deploy-to-eks:
    needs: build-and-push
    runs-on: ubuntu-latest
    env:
      CREATE_EKS: ${{ secrets.CREATE_EKS }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Check if EKS cluster exists
        id: check-cluster
        run: |
          if aws eks describe-cluster --name "${{ secrets.EKS_CLUSTER_NAME }}" --region "${{ secrets.AWS_REGION }}" >/dev/null 2>&1; then
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
          else
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Install eksctl
        if: ${{ env.CREATE_EKS == 'true' && steps.check-cluster.outputs.cluster_exists == 'false' }}
        run: |
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin

      - name: Create EKS cluster (if enabled and not exists)
        if: ${{ env.CREATE_EKS == 'true' && steps.check-cluster.outputs.cluster_exists == 'false' }}
        run: |
          chmod +x ./scripts/eksctl.sh
          CI=true ./scripts/eksctl.sh up ${{ secrets.EKS_CLUSTER_NAME }} ${{ secrets.AWS_REGION }} t3.small

      - name: Update kubeconfig for EKS
        run: |
          # Verify the cluster exists first to provide a clearer error message when it doesn't
          if ! aws eks describe-cluster --name "${{ secrets.EKS_CLUSTER_NAME }}" --region "${{ secrets.AWS_REGION }}" >/dev/null 2>&1; then
            echo "EKS cluster '${{ secrets.EKS_CLUSTER_NAME }}' not found in region '${{ secrets.AWS_REGION }}'."
            echo "Listing available clusters in region '${{ secrets.AWS_REGION }}':"
            aws eks list-clusters --region "${{ secrets.AWS_REGION }}" || true
            echo "\nPlease ensure the secret EKS_CLUSTER_NAME is set to an existing cluster, or create the cluster before running this workflow."
            exit 1
          fi

          aws eks update-kubeconfig --name "${{ secrets.EKS_CLUSTER_NAME }}" --region "${{ secrets.AWS_REGION }}"

      - name: Apply Kubernetes manifests
        run: kubectl apply -f k8s/deployment.yaml

      - name: Update deployments with new images
        run: |
          set -e
          kubectl -n default set image deployment/gocools-backend backend=${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_BACKEND }}:latest --record
          kubectl -n default set image deployment/gocools-frontend frontend=${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_FRONTEND }}:latest --record

      - name: Wait for rollouts
        run: |
          kubectl -n default rollout status deployment/gocools-backend --timeout=3m
          kubectl -n default rollout status deployment/gocools-frontend --timeout=3m
---
name: Node CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  backend-ci:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Use Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
      - name: Install backend dependencies
        run: |
          cd backend
          npm ci
      - name: Quick sanity check
        run: |
          cd backend
          # avoid requiring index.js (which starts a server in this project).
          # This step verifies installation completed and that the main file exists.
          test -f index.js && node -e "console.log('backend files present')"
